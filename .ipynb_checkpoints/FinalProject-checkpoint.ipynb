{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5015523-9b6c-4760-b065-df4815ece882",
   "metadata": {},
   "source": [
    "# The Machine Learning Project Lifecycle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba85c90-1b98-4e11-ba4c-f9a0db4fccc7",
   "metadata": {},
   "source": [
    "<img src='assets/ml_lifecycle.png' alt='ml_lifecycle.png'>\n",
    "\n",
    "In the final activity of this course, you will conduct a case study to apply everything you have learned about each stage of the machine learning (ML) project lifecycle. In particular, you will go through these stages:\n",
    "\n",
    "* **Scoping** - to outline the project's objectives, as well as examine its feasibility and value.\n",
    "* **Data** - to analyze the available data and determine how to improve its quality.\n",
    "* **Modeling** - to evaluate the performance of your model and prioritize what to work on.\n",
    "* **Deployment** - to plan for monitoring and maintaining your model in production.\n",
    "\n",
    "By walking through each step of this iterative process, you will get hands-on experience in applying the principles and best practices needed to run a successful ML project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77250934-f910-4b3c-a5ec-d45f54a708ab",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><font size=\"3\" color=\"green\"><b>Click here for some important notes about the structure of this lab</b></font></summary>\n",
    "\n",
    "  <br>\n",
    "  This lab is longer than the previous labs because it goes through all the stages of the ML Project Lifecycle. The Data, Modeling, and Deployment sections will each take around 20 to 30 minutes to complete depending on how much time you spend on the exercises.\n",
    "\n",
    "  <br>\n",
    "  <br>\n",
    "  \n",
    "  If you want to take a break and come back later, we've created checkpoints where you can continue your progress without re-running the entire lab. Each stage has a `Setup` subsection that loads your previous results. For example, if you're already done with the `Data` section but need to step out for a while, you can click on `Modeling` below and run the cells under `Setup` to load the results you got from the `Data` section. Similarly, if you're done with `Data` and `Modeling`, you only need to click on `Deployment` below and run the `Setup` cells to load the resources needed for that section.\n",
    "  \n",
    "  * [Data](#Data)\n",
    "  * [Modeling](#Modeling)\n",
    "  * [Deployment](#Deployment)\n",
    "\n",
    "  If you get stuck on the coding exercises, you can click the green font above the code cell to see hints or the solution.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53336525-3334-4d3e-bd78-d183ed9edb0e",
   "metadata": {},
   "source": [
    "## Case study\n",
    "\n",
    "A small start-up developing a news-related application wants to classify news articles collected from several sources. One of their previous developers built a lightweight prototype using ML, but in their internal evaluation, they observed that it's not working as well as they expected. They have brought you in to troubleshoot their processes and deploy an improved beta version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f0be8a-0e03-4276-b7f8-98d924da85c5",
   "metadata": {},
   "source": [
    "<a id='Scoping'></a>\n",
    "## Scoping\n",
    "\n",
    "Before diving into building the system, you'll want to first gather some details about the project. This information will help guide your decisions and set expectations for all the stakeholders. Here are the steps you might take:\n",
    "\n",
    "* **Identify a business problem** - The company needs a news article classifier to integrate into the application they are building. You'll want to identify the relevant business metrics and determine if the technology can indeed help improve those metrics. In this case, an ML model can help scale up the company's operations as they get more news sources. Manual sorting can't keep up with the velocity of the data, and traditional rule-based algorithms may have given subpar results.\n",
    "\n",
    "* **Brainstorm AI solutions** - This part has already been done for you. The previous developer settled on an AI solution that classified news articles using their title. In other projects, you may want to brainstorm a few different solutions to see which one addresses the business problem most effectively.\n",
    "\n",
    "* **Assess the feasibility and value of potential solutions** - ML has demonstrated excellent performance in the area of text classification, so you're confident that this project can be successful. The company already has a prototype, but they want to build something better. Here, you can ask about the data collection process and the problems they encountered with the initial deployment. To assess project ethics, you can also ask if there is an official agreement with the news sources, and if users are linked directly to the publisher's website if they click on an article. Determine the company's stance on fake news and other types of misinformation. Make sure this is a project that moves humanity forward and aligns with your personal ethical values.\n",
    "\n",
    "* **Determine milestones and budget for resourcing** - Identify the metrics for success and estimate the time and resources needed to carry out the project. Since the company is just starting to integrate ML into their workflow, they also want to better understand its capabilities. They have defined a simple model to predict an article's topic based only on its title. The initial model's results establish a baseline measure of performance as you improve the system. You committed 3 weeks to build and test a proof-of-concept based only on their existing prototype. This will help you estimate future timelines and have a feel for the resources you'll need to build a proper end-to-end project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc6c440-5e2e-446c-84b9-5cc933712cc7",
   "metadata": {},
   "source": [
    "<a id='Data'></a>\n",
    "## Data\n",
    "\n",
    "Recall from the lectures that in **data-centric AI development**, the quality of the data is paramount. You will take a data-centric approach in this project by initially holding the model code fixed and iteratively improving the data to achieve better model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a85bcf-0a6b-44eb-8bdc-17718a3ac6c0",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca90aad2-afd0-4829-9a71-00b7b71b04a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages you will need in the Data Stage\n",
    "\n",
    "import lab_utils\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e048bca-7370-43f1-84c4-9d7d46616637",
   "metadata": {},
   "source": [
    "In the Data stage, you  will look at the available data and see if it's sufficient for the application. The prototype files build by the previous developer are in the `E1` folder(short for Experiment 1), which has this structure:\n",
    "\n",
    "* `data` - CSV files for the train and test sets\n",
    "* `model` - a Tensorflow SavedModel that is trained on `train_data.csv` from the `data` folder\n",
    "* `vocab` -  two text files\n",
    "    * `labels.txt` - the 8 topics that the articles must be classified into\n",
    "    * `vocabulary.txt` - the top 10,000 most common words in the article titles from `train_data.csv`\n",
    " \n",
    "Run the code below to save the folder paths to variables so you can easily access them later. This code uses the `set_experiment_dirs()` helper function found in the `lab_utils.py` file. This utility file contains other functions that you will reuse several times in this notebook. Feel free to explore the functions in the said file to learn more about how they work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6478f75-1daf-42f7-95b6-43ede2a9bc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working folder for the experiment\n",
    "BASE_DIR = './E1'\n",
    "\n",
    "# Get the subdirectories that contain the experiment files\n",
    "data_dir, model_dir, vocab_dir = lab_utils.set_experiment_dirs(BASE_DIR)\n",
    "\n",
    "print(\n",
    "    f'base directory: {BASE_DIR}\\n\\n'\n",
    "    f'data: {data_dir}\\n'\n",
    "    f'model: {model_dir}\\n'\n",
    "    f'vocab: {vocab_dir}\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547a7793-91cc-43f6-b391-d432591c9f19",
   "metadata": {},
   "source": [
    "Now, you will load the datasets into Pandas dataframes so you can preview and manipulate them later. Each dataset has 6 columns, but only 2 are used by the prototype model:\n",
    "\n",
    "* `title` - The title of the article. This is the input to the model.\n",
    "* `topic` - The category of the article. This is the label the model is trying to predict. It has 8 classes: entertainment, health, technology, world, business, sports, nation, and science.\n",
    "\n",
    "The other 3 columns are only used in other parts of the company's app:\n",
    "* `link`\n",
    "* `domain`\n",
    "* `published_date`\n",
    "\n",
    "Run the cell below to preview the first 10 rows of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2475b8d-a79d-4406-8134-d485036d9860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the column width so you can see the entire length of the `title` column\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Load the datasets into dataframes\n",
    "train_df = pd.read_csv(f'{data_dir}/train_data.csv')\n",
    "test_df = pd.read_csv(f'{data_dir}/test_data.csv')\n",
    "\n",
    "# Preview the first 10 rows of the training set\n",
    "train_df[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad2eb30-7847-4761-9de5-ca22efc15260",
   "metadata": {},
   "source": [
    "### Define the Data\n",
    "\n",
    "Earlier in this course, you learned that preparing the data effectively is key to good model performance. If the human labellers employed to provide the ground truth for the dataset were working with ambiguous labelling instructions, they may produce inconsistent labels. These inconsistent labels make it harder for the learning algorithm to understand the relationship between the inputs and outputs.\n",
    "\n",
    "Given the information you have so far about this project, write down your questions or observations about the data (particularly the input feature and label). Where might there be sources of ambiguity? What changes could help your model make good predictions? Use the code cell below to explore the datasets. If you see something wrong, don't fix it just yet. You'll have the chance to make any fixes later in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74714c8-9522-40dc-afb1-0dcabaf3ebc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use this cell to explore the dataframes. The cell below\n",
    "# gets the titles and topics. Feel free to run other commands.\n",
    "train_df[['title', 'topic']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc3f3ad-1226-456f-9e1e-75c693646079",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "👉 **EXERCISE:** Another thing you can try is to get a few rows and label them yourself. You can just write them on a piece of paper or a text file on your computer. This can help you develop some intuition regarding the data. You can run the cell below to get 10 titles with the labels hidden. Try classifying them manually and see if it matches those in the topic column (uncomment the 2nd line to see it). Again, the 8 classes you can choose from are: entertainment, health, technology, world, business, sports, nation, and science.\n",
    "\n",
    "When you're done, you can compare your manual labels to those already in the dataset. If you found some mismatches, think about why that is. Perhaps you'll get some insight on how to carry out the data collection, and clarify the labelling process in the future to avoid inconsistencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea7b8c2-9c72-4f97-ad80-e225fb91829e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE: print 10 titles and manually label them\n",
    "# NOTE: Please do not overwrite the dataframe. Type your answers in a text file or write them on scrap paper\n",
    "\n",
    "# Indices of the dataframe to use\n",
    "start_index = 30\n",
    "end_index = 40\n",
    "\n",
    "# Sample titles to label\n",
    "train_df[['title']][start_index:end_index]\n",
    "\n",
    "# # When you're done, uncomment the next line to see the 'true' labels \n",
    "# train_df[['title', 'topic']][start_index:end_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568b5632-346a-4b73-8be1-481cd3b4a7ba",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "When you're done visually inspecting the data and writing down your thoughts, you can click on the green text below to see some example questions you may want to ask given this type of dataset.\n",
    "\n",
    "<details>\n",
    "  <summary><font size=\"2\" color=\"green\"><b>Click here for some example questions about the dataset</b></font></summary>\n",
    "  \n",
    "  * <i>How were the labels determined? How many labellers were involved and how were their outputs aggregated? What is the inter-rater reliability?\n",
    "  * <i>How is each topic defined? For example, how are Science articles different from Technology ones?\n",
    "  * <i>Can any of the other 3 unused columns be used as a predictive feature in the model?\n",
    "  * <i>Is the article's title really enough for this application? If I could collect data again, what other predictive features would I want to have in order to generate more consistent labels?\n",
    "  * <i>Will it help accuracy to add a new topic, or to merge two overlapping topics?\n",
    "  \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798ad9ba-a061-4b20-b37e-31da63f05f04",
   "metadata": {},
   "source": [
    "You don't have to address your questions right away. For some questions, like whether you can collect more data, you may not be able to get an answer easily. For now, continue to the next section to gather more insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e18f33-e51e-4c81-811e-31e8be07615e",
   "metadata": {},
   "source": [
    "### Establish a Baseline\n",
    "\n",
    "Establishing a performance baseline early on allows you to better understand whether your updated model is an improvement, and if so, by how much. You already saw some ways to establish a baseline in the lectures. For this project, you will use the performance of the company's existing prototype as your baseline. \n",
    "\n",
    "The team can't find the previous metrics (it seems the previous developer left in a hurry), so you'll have to calculate them yourself. You will want to load the model and feed the test data. The model is built in Tensorflow, so you will use its API in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c35a41-0ac3-49d8-8848-7bea21405217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = tf.keras.models.load_model(model_dir)\n",
    "\n",
    "# Show the model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561b0524-2042-4037-bd84-0b00b6d5cdcd",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "As you can see, the model architecture is relatively simple, mainly using an Embedding and Dense layer to classify the text. It's also lightweight with only about 240k parameters. You can also check the optimizer, loss, and metrics used. The output below shows `Adam`, `sparse_categorical_crossentropy` and `sparse_categorical_accuracy`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41780d1f-7599-44ac-851f-820fa0f72419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get info about compiling the model\n",
    "model.get_compile_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d0ebc3-3ad2-4a3e-9ea0-8a6d37d295fe",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "You can use the model's `evaluate()` method to get its accuracy given the test data. However, the `test_df` dataframe still uses strings for both the input features (`test_df['title']`) and labels (`test_df['topic']`). The model can only consume numbers so you need to transform the strings into such. You can convert the news titles to integer sequences, and the labels to integers. The files in the `vocab_dir` can help with that.\n",
    "\n",
    "First, you can use a [StringLookup()](https://www.tensorflow.org/api_docs/python/tf/keras/layers/StringLookup) layer like below to convert the labels to numerical indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6de21d2-1c92-4187-8661-908c16de54ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a lookup list for the labels\n",
    "topic_lookup = tf.keras.layers.StringLookup(vocabulary=f'{vocab_dir}/labels.txt', num_oov_indices=0)\n",
    "\n",
    "# Check the list of labels\n",
    "topic_lookup.get_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b74722-db22-4836-aeee-8e0c6bb1b5de",
   "metadata": {},
   "source": [
    "Next, to convert the string titles to numeric features, you'll need to tokenize them and generate integer sequences for each title. This can be done with a [TextVectorization()](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization) layer that uses the `vocabulary.txt` file in `vocab_dir`. It contains the top 10,000 commonly used words of the training set. The team already established that titles rarely have more than 20 words, so anything after that can be dropped. \n",
    "\n",
    "You can create the layer given the settings above as shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dadb38-b09d-45bd-88de-8b9e85bc9fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Title length and vocabulary size used by the team for the prototype\n",
    "MAX_LENGTH = 20\n",
    "VOCAB_SIZE = 10000\n",
    "\n",
    "# Instantiate a layer for text preprocessing\n",
    "title_preprocessor = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE, output_sequence_length=MAX_LENGTH)\n",
    "\n",
    "# Load the vocabulary file\n",
    "title_preprocessor.load_assets(vocab_dir)\n",
    "\n",
    "# Check the vocabulary size\n",
    "print(f'vocabulary size: {title_preprocessor.vocabulary_size()}')\n",
    "\n",
    "# Get a sample title\n",
    "sample_title = train_df['title'][10]\n",
    "\n",
    "# Sample title in string format\n",
    "print(f\"sample text: {sample_title}\")\n",
    "\n",
    "# Sample title represented as an integer sequence\n",
    "print(f\"sample text (preprocessed): {title_preprocessor(sample_title)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6e750d-659a-4cdd-ac91-75a47413647e",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "You can then get the evaluation metrics by running the code below. We provided a `df_to_tfdata` utility so you can convert dataframes to a `tf.data.Dataset` that has integer sequences given the `title_preprocessor` and `topic_lookup` layers you just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a439389-f379-4186-9eee-66f7ac868cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the test dataframe to a tf dataset\n",
    "test_ds = lab_utils.df_to_tfdata(test_df, topic_lookup, title_preprocessor)\n",
    "\n",
    "# Get the metrics\n",
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652d6271-4f87-4503-883d-bbe3e39ba434",
   "metadata": {},
   "source": [
    "[SparseCategoricalAccuracy()](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/SparseCategoricalAccuracy) measures how often the predictions match integer labels. As shown above, the current model has 78% accuracy across all rows in the test dataset. You'll examine and build upon this in the next steps.\n",
    "\n",
    "It's best to have some method to track your experiments as you advance through a project. It can be as simple as a text file that tracks important metrics and points to relevant directories. You can use the `experiment-tracker.csv` file in the file explorer on the left to write the results (Right Click and Select `Open With` --> `Editor` to make changes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d753ec1-9d1a-49a3-a285-4051f427108a",
   "metadata": {},
   "source": [
    "### Label and Organize Data\n",
    "\n",
    "You just made a quick and dirty run-through of the prototype files and are able to measure the current model's performance. However, following the principles of **data-centric AI development**, you'd like to check if the data is well-organized before you do more experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716b817b-f4ec-4a24-ab36-717ba19694f6",
   "metadata": {},
   "source": [
    "#### Data Pipeline\n",
    "\n",
    "Being mindful of the steps that your data goes through before it reaches the model will help ensure replicability of your results and avoid erratic data transformations when you push the model to production.\n",
    "\n",
    "Taking extensive notes about your experiments helps keep track of these steps as your pipeline becomes more complex. You can start by listing the steps from raw data to predictions, creating a flowchart like the diagram below. Then, add more details about each step (e.g. locations of the vocabulary files, etc) to record important aspects of your pipeline.\n",
    "\n",
    "<center><img src='assets/pipeline_diagram.png' alt='pipeline_diagram.png'></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2a9c69-d3d5-4628-97d5-5e6c25d01a39",
   "metadata": {},
   "source": [
    "#### Create a balanced train/dev/test split\n",
    "\n",
    "Another thing you can look at is the data splits. You learned in the lectures that it's best for small datasets to have a balanced split between all categories across the train, dev, and test sets.\n",
    "\n",
    "The next two cells will show the percentage of each category in the train and test sets below using the `value_counts()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e918fe-de0d-4035-bd2a-d04c5c17c421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the percentage of each class in the train set\n",
    "train_df.topic.value_counts(normalize=True).sort_index().mul(100).round(1).astype(str) + '%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57da0793-b107-4182-9373-471794fdbc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the percentage of each class in the test set\n",
    "test_df.topic.value_counts(normalize=True).sort_index().mul(100).round(1).astype(str) + '%'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c061da5-2c87-4fd9-aa03-11f3559ece2f",
   "metadata": {},
   "source": [
    "As you can see, the percentage of each class is not the same between the train and test sets. The most extreme case is the `SCIENCE` category, where there are a lot more examples in the test set compared with the training set. Because of this imbalance, the learning algorithm might not find a lot of patterns for the science-related titles and will likely underperform compared to the rest of the topics.\n",
    "\n",
    "One strategy you can use to get a better balance between the topics is to recombine the train and test sets into one pool, then split them again in a more balanced way. You'll try that approach in the exercise below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ea1d05-fe4f-48bb-a8a9-d4c074a76066",
   "metadata": {},
   "source": [
    "👉 **EXERCISE:** Combine the train and test dataframes you've been using, and produce a balanced split of the `topic` column for three datasets:\n",
    "\n",
    "* train_df - 60% of the combined datasets\n",
    "* dev_df - 20%\n",
    "* test_df - 20%\n",
    "\n",
    "Notice that you will now have a dev set, whereas the previous experiment only has train and test sets. You will use this to monitor the validation accuracy when you retrain your models.\n",
    "\n",
    "<details>\n",
    "  <summary><font size=\"2\" color=\"green\"><b>Click here for hints</b></font></summary>\n",
    "    \n",
    "  * Refer to the Week 2 ungraded lab and see how to produce balanced splits given a dataframe column name.\n",
    "  * You will use an argument of the [train_test_split()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function\n",
    "    \n",
    "  <details>\n",
    "    <summary><font size=\"2\" color=\"green\"><b>Click here for the solution</b></font></summary>\n",
    "    <br>\n",
    "      \n",
    "```\n",
    "      \n",
    "    # Load the train and test sets\n",
    "    train_df = pd.read_csv(f'{data_dir}/train_data.csv')\n",
    "    test_df = pd.read_csv(f'{data_dir}/test_data.csv')\n",
    "    \n",
    "    # Combine the two datasets. Set ignore_index to False.\n",
    "    combined_df = pd.concat([train_df,test_df], ignore_index=True)\n",
    "    \n",
    "    # Split the combined dataset to 60% train, 20% dev, and 20% test set. Produce a balanced split along the `topic` column.\n",
    "    train_df, test_df = train_test_split(combined_df, test_size=0.2, stratify=combined_df['topic'])\n",
    "    train_df, dev_df = train_test_split(train_df, test_size=0.25, stratify=train_df['topic'])\n",
    "```\n",
    "\n",
    "  </details>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899fd2e6-ce42-48ec-aaa0-19681bfeccb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "\n",
    "# Load the train and test sets\n",
    "train_df = pd.read_csv(f'{data_dir}/train_data.csv')\n",
    "test_df = pd.read_csv(f'{data_dir}/test_data.csv')\n",
    "\n",
    "# Combine the two datasets. Set ignore_index to False.\n",
    "combined_df = pd.concat([train_df,test_df], ignore_index=True)\n",
    "\n",
    "### START CODE HERE ###\n",
    "\n",
    "# Split the combined dataset to 60% train, 20% dev, and 20% test set. Produce a balanced split along the `topic` column.\n",
    "train_df, test_df = None\n",
    "train_df, dev_df = None\n",
    "\n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294b180a-eaf7-4218-a249-8744c2684e44",
   "metadata": {},
   "source": [
    "👉 **EXERCISE:** Show the percentage of each topic for the train, dev, and test sets. You should see this output for all three sets if you did the data splits correctly:\n",
    "\n",
    "```\n",
    "topic\n",
    "BUSINESS         13.8%\n",
    "ENTERTAINMENT    13.8%\n",
    "HEALTH           13.8%\n",
    "NATION           13.8%\n",
    "SCIENCE           3.5%\n",
    "SPORTS           13.8%\n",
    "TECHNOLOGY       13.8%\n",
    "WORLD            13.8%\n",
    "Name: proportion, dtype: object\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1fa40d-3dca-4ff1-8637-ce5b9e2de92f",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><font size=\"2\" color=\"green\"><b>Click here for hints</b></font></summary>\n",
    "    * You can use the same code in the previous cells when you only had a train and test set.\n",
    "    <br>\n",
    "    <br>\n",
    "  <details>\n",
    "    <summary><font size=\"2\" color=\"green\"><b>Click here for the solution</b></font></summary>\n",
    "    <br>\n",
    "\n",
    "```\n",
    "# Print the percentage of each class in the train set\n",
    "train_df.topic.value_counts(normalize=True).sort_index().mul(100).round(1).astype(str) + '%'\n",
    "\n",
    "# Print the percentage of each class in the dev set\n",
    "dev_df.topic.value_counts(normalize=True).sort_index().mul(100).round(1).astype(str) + '%'\n",
    "\n",
    "# Print the percentage of each class in the test set\n",
    "test_df.topic.value_counts(normalize=True).sort_index().mul(100).round(1).astype(str) + '%'\n",
    "```\n",
    "      \n",
    "  </details>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a58e2e9-990a-43c9-88e0-85ae37c09bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE: Print the percentage of each class in the train set\n",
    "\n",
    "### START CODE HERE ###\n",
    "None\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811481a8-cf5a-4189-99a5-8c891a55cd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE: Print the percentage of each class in the dev set\n",
    "\n",
    "### START CODE HERE ###\n",
    "None\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06970db3-8e26-453d-b4bd-35cc718fffb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE: Print the percentage of each class in the test set\n",
    "\n",
    "### START CODE HERE ###\n",
    "None\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a64b6ab-b9b9-402d-bf96-c04cf04f8ee7",
   "metadata": {},
   "source": [
    "One thing to note here is that there are fewer science articles in the dataset compared with the rest of the topics. It's not uncommon to see this imbalance in practice. For example, if you're working on medical diagnosis with image datasets, you might have a lot more negative examples (i.e. healthy patients) than positive ones. If you have the time and resources, you can try to find more data or create synthetic data to augment the examples in this category. If not, you will have to pay attention to the metrics on this category when evaluating your model, which you will do later in this project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c2950d-2009-4299-85d9-db37b59a3ba1",
   "metadata": {},
   "source": [
    "Since you're doing a quick iteration, you will skip augmenting the data for now and use the splits above for the Modeling stage. It's a good idea to have a backup of the datasets so you can track which ones are used when you run an experiment.\n",
    "\n",
    "You'll create a new directory `E2` (meaning \"Experiment 2\") to track this second iteration of the data and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999fb544-36c1-43a0-b6be-d67c107acddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the base directory for the second experiment\n",
    "BASE_DIR = './E2'\n",
    "\n",
    "# Set the subdirectories that will contain the experiment files\n",
    "data_dir, model_dir, vocab_dir = lab_utils.set_experiment_dirs(BASE_DIR)\n",
    "\n",
    "# Save the datasets\n",
    "lab_utils.save_data(train_df, data_dir, 'train_data.csv')\n",
    "lab_utils.save_data(dev_df, data_dir, 'dev_data.csv')\n",
    "lab_utils.save_data(test_df, data_dir, 'test_data.csv')\n",
    "\n",
    "# Save the labels\n",
    "lab_utils.save_labels(topic_lookup, vocab_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c47ad0-2689-48fc-96e9-5e2adb3b518f",
   "metadata": {},
   "source": [
    "<a id='Modeling'></a>\n",
    "## Modeling\n",
    "\n",
    "In the Modeling stage, you will select a model to learn from your data. You will evaluate its performance and perform error analysis to know how it can be improved. As a reminder, we are now here in the product lifecyle:\n",
    "\n",
    "<img src='assets/ml_lifecycle_modeling.png' alt='ml_lifecycle_modeling.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d189ec3-fdcb-4819-9c76-77607473553b",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c65c02b-b74b-41b4-881e-e1c92198fbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lab_utils\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Working folder for the experiment\n",
    "BASE_DIR = './E1'\n",
    "\n",
    "# Get the subdirectories that contain the experiment files\n",
    "_, model_dir, _ = lab_utils.set_experiment_dirs(BASE_DIR)\n",
    "\n",
    "# Load the model\n",
    "model = tf.keras.models.load_model(model_dir)\n",
    "\n",
    "# Working folder for the experiment\n",
    "BASE_DIR = './E2'\n",
    "\n",
    "# Title length and vocabulary size used by the team for the prototype\n",
    "MAX_LENGTH = 20\n",
    "VOCAB_SIZE = 10000\n",
    "\n",
    "# Get the subdirectories that contain the experiment files\n",
    "data_dir, model_dir, vocab_dir = lab_utils.set_experiment_dirs(BASE_DIR)\n",
    "\n",
    "# Load the train and test sets\n",
    "train_df = pd.read_csv(f'{data_dir}/train_data.csv')\n",
    "dev_df = pd.read_csv(f'{data_dir}/dev_data.csv')\n",
    "test_df = pd.read_csv(f'{data_dir}/test_data.csv')\n",
    "\n",
    "# Instantiate a layer for text preprocessing\n",
    "title_preprocessor = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE, output_sequence_length=MAX_LENGTH)\n",
    "\n",
    "# Create a lookup list for the labels\n",
    "topic_lookup = tf.keras.layers.StringLookup(vocabulary=f'{vocab_dir}/labels.txt', num_oov_indices=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4495e64-eb25-4a34-a7bd-67cfcfefdeee",
   "metadata": {},
   "source": [
    "### Select and train the model\n",
    "\n",
    "Since you're practicing **data-centric AI development**, you want to first do a quick analysis of the prototype model with your improved dataset before making changes to the model. You will use the same model architecture and feed in the datasets you generated above. However, you can't do that right away because the model you loaded earlier was trained on the imbalanced split of the topics.\n",
    "\n",
    "You will first need to update your title preprocessor with a new vocabulary that represents the new training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04942a01-fc20-4f93-bc95-1ccdc2d0c7c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract the titles from the new training set\n",
    "train_inputs = train_df['title']\n",
    "\n",
    "# Generate a new vocabulary\n",
    "title_preprocessor.adapt(train_inputs)\n",
    "\n",
    "# Save the new vocabulary\n",
    "lab_utils.save_vocab(title_preprocessor, vocab_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbec55b-0972-42bc-98ab-59b6e63e5844",
   "metadata": {},
   "source": [
    "You can now train and evaluate the model with the new datasets. You can instantiate a new one with the same architecture or in this case, reinitialize the weights of an existing model so you can train from scratch. There's a `model_reset_weights()` helper function in `lab_utils` that will work for the simple prototype model.\n",
    "\n",
    "This training should take about 1 minute to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36e8bc2-10a3-414f-a2bb-5b7ea2703858",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 5\n",
    "\n",
    "# Convert the string datasets to Tensorflow datasets\n",
    "train_ds = lab_utils.df_to_tfdata(train_df, topic_lookup, title_preprocessor, shuffle=True)\n",
    "dev_ds = lab_utils.df_to_tfdata(dev_df, topic_lookup, title_preprocessor)\n",
    "test_ds = lab_utils.df_to_tfdata(test_df, topic_lookup, title_preprocessor)\n",
    "\n",
    "# Reset the model weights\n",
    "model = lab_utils.model_reset_weights(model)\n",
    "\n",
    "# Train the model. Use the dev set to check if your model is overfitting.\n",
    "model.fit(train_ds, epochs=NUM_EPOCHS, validation_data=dev_ds, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d783251c-2286-44ff-ac97-6fdfab6c73d0",
   "metadata": {},
   "source": [
    "After training, you can evaluate the model with the test set and log the results in the `experiment-tracker.csv` file. You should also save the model in the `E2` folder as a backup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89613e2-2c36-47df-a788-a379bb267195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the loss and metrics\n",
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db6a5bf-bc69-4c1b-b575-9eea0047e57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e106847c-694c-402b-b5ed-3393ad64e354",
   "metadata": {},
   "source": [
    "Taking a look at the model performance, you'll notice that the overall accuracy has improved from about 78% to about 81% just from the changes you made to the dataset already. But is this accuracy all it seems? In the next section, you'll examine this accuracy more closely based on slices of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3981df76-9204-4760-a212-2b650ac78ffb",
   "metadata": {},
   "source": [
    "### Perform Error Analysis\n",
    "\n",
    "The first iteration of your model will likely underperform and you will need to make adjustments to make it better. Error analysis helps you determine which part of the process you need to tweak to give the biggest improvement. Likewise, it helps you avoid focusing on parts that do not greatly affect the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f18003e-8e21-4775-a868-5d3622552edf",
   "metadata": {},
   "source": [
    "#### Prioritizing What to Work On\n",
    "\n",
    "Looking at the performance of your model on different categories of the data will help you decide how to improve its performance. In this case, you will evaluate the model on each of the 8 classes it's trying to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af57786-448f-4de0-96c5-2bad5f09ccc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of topics\n",
    "topics = topic_lookup.get_vocabulary()\n",
    "\n",
    "# Evaluate the model's performance for each topic\n",
    "lab_utils.print_metric_per_topic(dev_df, topics, topic_lookup, title_preprocessor, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3013cb69-8ce9-4967-acd1-3c4e1d83d97d",
   "metadata": {},
   "source": [
    "From the results, you can check which ones stand out. If you have a baseline such as human-level performance (HLP), you can measure how far each category is from that value, then focus your efforts on the category that will bring the biggest overall improvement.\n",
    "\n",
    "On the other hand, this analysis can also help you spot errors. You might notice that performance on the `BUSINESS` topic seems suspiciously high compared to the rest. See if you can find why that is. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799b648e-e9dd-4526-9874-618dbef28f0d",
   "metadata": {},
   "source": [
    "👉 **EXERCISE:** Use the code cell below to extract the business articles and investigate the input features.\n",
    "\n",
    "<details>\n",
    "  <summary><font size=\"2\" color=\"green\"><b>Click here for hints</b></font></summary>\n",
    "    * This will only take 1 line. Print the `train_df` rows that only has the topic equal to `BUSINESS`\n",
    "    <br>\n",
    "    <br>\n",
    "  <details>\n",
    "    <summary><font size=\"2\" color=\"green\"><b>Click here for the solution</b></font></summary>\n",
    "    <br>\n",
    "\n",
    "```\n",
    "# filter the training dataframe's business articles\n",
    "train_df[train_df.topic=='BUSINESS']\n",
    "```\n",
    "      \n",
    "  </details>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b7b9b7-17d1-44d5-a1ff-01ffede04ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE: \n",
    "\n",
    "### START CODE HERE\n",
    "\n",
    "# filter the training dataframe's business articles\n",
    "None\n",
    "\n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b985bc2-1917-4749-ba91-2e64c6973ae0",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "You might have noticed that the titles for all articles are the same: `\"Why iPhone 12 Will Be Another 'Defining Chapter' In Apple's Growth Story\"`. The model only learned this pattern, so it will likely not generalize well when real-world business-related titles come in. \n",
    "\n",
    "After some investigation, you find out that the previous developer accidentally overwrote the columns while fixing a character encoding. Luckily, there is a backup file which contains the original values. You can now procede with a new experiment using the correct values. To do so, generate train, dev, and test sets again and save these datasets to a folder named `E3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e86de1b-e4a2-4762-81f9-45b60efbfbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the experiment folder\n",
    "BASE_DIR = './E3'\n",
    "\n",
    "# Set the subdirectories that will contain the experiment files\n",
    "data_dir, model_dir, vocab_dir = lab_utils.set_experiment_dirs(BASE_DIR)\n",
    "\n",
    "# Load the backup CSV\n",
    "combined_df = pd.read_csv(f'./.backup.csv')\n",
    "\n",
    "# Generate train, dev, and test sets as you did before.\n",
    "train_df, test_df = train_test_split(combined_df, test_size=0.2, stratify=combined_df['topic'])\n",
    "train_df, dev_df = train_test_split(train_df, test_size=0.25, stratify=train_df['topic'])\n",
    "\n",
    "# Save the datasets under the E3 folder\n",
    "lab_utils.save_data(train_df, data_dir, 'train_data.csv')\n",
    "lab_utils.save_data(dev_df, data_dir, 'dev_data.csv')\n",
    "lab_utils.save_data(test_df, data_dir, 'test_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0b1ca0-8cc1-4740-8330-a422115ad90f",
   "metadata": {},
   "source": [
    "You will also need to update the title preprocessor and generate a new vocabulary. Do this in the cell below and save the new vocabulary in the E3 folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234cc7a5-fe61-4d6c-9e58-7b6a13f9d415",
   "metadata": {},
   "source": [
    "👉 **EXERCISE:** Use the code cell below to generate a vocabulary based on the training set. Save the vocabulary in the `vocab_dir` folder.\n",
    "\n",
    "<details>\n",
    "  <summary><font size=\"2\" color=\"green\"><b>Click here for hints</b></font></summary>\n",
    "    * Get the titles from the train_df.\n",
    "    * Use the `adapt()` method of the `title_preprocessor` to generate the vocabulary.\n",
    "    * Use the `save_vocab()` utility from `lab_utils` to save the vocabulary.\n",
    "    <br>\n",
    "    <br>\n",
    "  <details>\n",
    "    <summary><font size=\"2\" color=\"green\"><b>Click here for the solution</b></font></summary>\n",
    "    <br>\n",
    "\n",
    "```\n",
    "# Generate a new vocabulary based on the new training set\n",
    "train_inputs = train_df['title']\n",
    "title_preprocessor.adapt(train_inputs)\n",
    "\n",
    "# Save the new vocabulary and labels\n",
    "lab_utils.save_vocab(title_preprocessor, vocab_dir)\n",
    "lab_utils.save_labels(topic_lookup, vocab_dir)\n",
    "```\n",
    "      \n",
    "  </details>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0fdbdd-e09a-41c6-8800-4426085651f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "\n",
    "### START CODE HERE ###\n",
    "\n",
    "# Generate a new vocabulary based on the new training set\n",
    "train_inputs = None\n",
    "None\n",
    "\n",
    "# Save the new vocabulary and labels\n",
    "None\n",
    "None\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd891f2-9abf-4d66-bd9b-7014c06ade8b",
   "metadata": {},
   "source": [
    "👉 **EXERCISE:** Now, convert the dataframes to numeric features and train the model again for 5 epochs. \n",
    "\n",
    "<details>\n",
    "  <summary><font size=\"2\" color=\"green\"><b>Click here for hints</b></font></summary>\n",
    "    * Use `lab_utils.df_to_tfdata` to convert the dataframes to `tf.data.Dataset`.\n",
    "    * Train the model for 5 epochs. Use `dev_df` as validation data.\n",
    "    <br>\n",
    "    <br>\n",
    "  <details>\n",
    "    <summary><font size=\"2\" color=\"green\"><b>Click here for the solution</b></font></summary>\n",
    "    <br>\n",
    "\n",
    "```\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "# Convert the dataframes to numeric features. Remember to shuffle the training set.\n",
    "train_ds = lab_utils.df_to_tfdata(train_df, topic_lookup, title_preprocessor, shuffle=True)\n",
    "dev_ds = lab_utils.df_to_tfdata(dev_df, topic_lookup, title_preprocessor)\n",
    "test_ds = lab_utils.df_to_tfdata(test_df, topic_lookup, title_preprocessor)\n",
    "\n",
    "# Reset the model weights\n",
    "model = lab_utils.model_reset_weights(model)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_ds, epochs=NUM_EPOCHS, validation_data=dev_ds, verbose=1)\n",
    "```\n",
    "      \n",
    "  </details>\n",
    "</details>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a1b61a-a842-442a-a954-83739fdefbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "\n",
    "### START CODE HERE ###\n",
    "\n",
    "NUM_EPOCHS = None\n",
    "\n",
    "# Convert the dataframes to numeric features. Remember to shuffle the training set.\n",
    "train_ds = None\n",
    "dev_ds = None\n",
    "test_ds = None\n",
    "\n",
    "# Reset the model weights\n",
    "model = None\n",
    "\n",
    "# Train the model\n",
    "None\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440600e6-5fab-43a7-a861-06e4d06fb205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set and write the results on the experiment tracker\n",
    "model.evaluate(test_ds)\n",
    "\n",
    "# Save the model to model_dir\n",
    "model.save(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535d3f07-204f-477d-8167-36c126909687",
   "metadata": {},
   "source": [
    "👉 **EXERCISE:** Now evaluate the model again on each topic. You should see the accuracy on the business articles drop from 100% because the model has to learn more words related to the topic.\n",
    "\n",
    "<details>\n",
    "  <summary><font size=\"2\" color=\"green\"><b>Click here for hints</b></font></summary>\n",
    "    * Use `lab_utils.print_metric_per_topic()` to print the accuracy per topic on the `dev_df` dataframe\n",
    "    <br>\n",
    "    <br>\n",
    "  <details>\n",
    "    <summary><font size=\"2\" color=\"green\"><b>Click here for the solution</b></font></summary>\n",
    "    <br>\n",
    "\n",
    "```\n",
    "lab_utils.print_metric_per_topic(dev_df, topics, topic_lookup, title_preprocessor, model)\n",
    "```\n",
    "      \n",
    "  </details>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ba59c7-98a9-4c16-a420-822a38b3bf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "\n",
    "### START CODE HERE ###\n",
    "\n",
    "None\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3de1808-ef01-43a6-97f6-8418e4e24abe",
   "metadata": {},
   "source": [
    "### Label Ambiguity and Raising HLP\n",
    "\n",
    "After fixing the previous error with the `BUSINESS` topic, you can now look at the other categories and see which ones can be further improved. You might see that the `WORLD` and `NATION` topics have the lowest accuracy so you may want to investigate that discrepancy. You can use the `get_errors()` helper function under the `lab_utils` to help filter errors. The code below will grab examples that the model incorrectly predicted to be `NATION` articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac41ec7-e4fe-4ded-8be3-12694d667650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get examples in the dev set that predicted `NATION` but the ground truth label is different\n",
    "lab_utils.get_errors(model, dev_df, title_preprocessor, topic_lookup, 'NATION')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f70e23-71eb-4522-bb59-afc096c07ffd",
   "metadata": {},
   "source": [
    "Although some predictions are indeed mistakes, you might notice that some examples might also be related to two categories. For example, this title: `COVID-19 hospital admissions up slightly across St. Louis area` sounds like it can both be a `HEALTH` and `NATION` article. \n",
    "\n",
    "You need to ask if the human labellers who provided the ground truth have clear instructions on how to label such topics. If some of them label COVID articles as `HEALTH` while others pick `NATION`, then this ambiguity will likely affect the model negatively.\n",
    "\n",
    "If a clear rule for choosing a single topic cannot be clearly defined, one way you can improve human-level performance is to allow labelers to select more than one topic. So, instead of just having this table when labelling:\n",
    "\n",
    "| Title      | Topic |\n",
    "| -----------| ----- |\n",
    "| Title 1    |       |\n",
    "| Title 2    |       |\n",
    "| Title 3    |       |\n",
    "\n",
    "They can have something like this instead where they can mark several categories for a title:\n",
    "\n",
    "| Title      | ENTERTAINMENT | HEALTH | TECHNOLOGY | WORLD | BUSINESS | SPORTS | NATION | SCIENCE |\n",
    "| -----------| ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- |\n",
    "| Title 1    |       |       |       |       |       |       |       |       |\n",
    "| Title 2    |       |       |       |       |       |       |       |       |\n",
    "| Title 3    |       |       |       |       |       |       |       |       |\n",
    "\n",
    "One other approach is to merge certain topics that are related to each other. So instead of having 8 classes, you can decide to only have 6: `ENTERTAINMENT`, `HEALTH`, `BUSINESS`, `SPORTS`, `WORLD and NATION` and `SCIENCE and TECHNOLOGY`. \n",
    "\n",
    "When making decisions like these, you need to get buy-in from the product/business owner because this will also impact other aspects of their operations. For example, this might mean that the article will appear in several parts of the News App, or their current system might break because some categories no longer exist.\n",
    "\n",
    "As a proof-of-concept, let's check if the second to the top class prediction of the model corresponds to the ground truth labels. The model is originally compiled to only get the top prediction of the softmax output. You can recompile the model to reward it if it the ground truth is in the top two predictions. You can use the [SparseTopKCategoricalAccuracy](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/SparseTopKCategoricalAccuracy) metric for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052ca2c2-224d-47d2-99c3-ca7f3fe71885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the top-K accuracy to 2\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=[tf.keras.metrics.SparseTopKCategoricalAccuracy(k=2)]             \n",
    "             )\n",
    "\n",
    "# Check the accuracy\n",
    "model.evaluate(dev_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd9933e-1399-4499-afdd-4e6ac86e86a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print the accuracy per topic\n",
    "lab_utils.print_metric_per_topic(dev_df, topics, topic_lookup, title_preprocessor, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fbc86e-3a51-410a-8b42-979a5c77a09e",
   "metadata": {},
   "source": [
    "It looks like the model already has good second guesses because the accuracy is improved across all categories. The `WORLD` and `NATION` topics had the biggest improvements with over 20 percentage points. You can further investigate why the model picks a different topic as the top prediction and see if the two approaches mentioned above could improve accuracy overall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b8e1e1-52d3-477b-a7ac-b76469191ed5",
   "metadata": {},
   "source": [
    "### Adding data\n",
    "\n",
    "At this point, you can also explore whether adding more data will help boost your model's performance. As you learned in the lecture, adding more data is much more likely to improve model performance than to damage it. \n",
    "\n",
    "As you saw earlier in this project, there are far fewer `SCIENCE` titles compared with the other categories, which might explain why the model is performing relatively poorly on `SCIENCE` articles even if it considers the top 2 predictions. There are some techniques for augmenting text data that you can try on your dataset ([such as synonym replacement, random insertion, random swap, and random deletion](https://arxiv.org/abs/1901.11196)). Just remember to only augment the training set, and not the dev and test sets.\n",
    "\n",
    "You can also try getting training examples from other datasets to see if having more diverse examples will help the model will pick up more common words used in the each category. You can find other open source news classification datasets online, such as [the AG News Classification Dataset](https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset) and [this dataset of headlines from the Huffington Post](https://www.kaggle.com/datasets/rmisra/news-category-dataset).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b828235-74e0-49b0-95a1-e80dc894b82d",
   "metadata": {},
   "source": [
    "### Modify the Model Parameters\n",
    "\n",
    "Lastly, you can modify the model parameters to observe the effects on model performance. Below you will find the code to build the prototype model. You can try adding more layers or just modifying the dimensionality of the embedding and dense layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6e7b55-1c6f-4217-bd8e-b8559f2de49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "EMBEDDING_DIM = 24\n",
    "DENSE_DIM = 24\n",
    "topic_size = topic_lookup.vocabulary_size()\n",
    "\n",
    "# Model Definition\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LENGTH),\n",
    "    tf.keras.layers.Dense(DENSE_DIM, activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(topic_size, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c319fc-c5a2-4cb7-a66f-5ebf40242415",
   "metadata": {},
   "source": [
    "You can also look at the [different arguments](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization) (e.g. `max_tokens`) of the `TextVectorization` layer which you used to preprocess the text. Another related option is to improve the vocabulary by removing stop words (such as `a`, `the`, `is`, and `are`) so that only meaningful words are fed into the model.\n",
    "\n",
    "Another way is to try a completely different architecture or a pre-trained model. [Here's one resource](https://keras.io/examples/nlp/) that demonstrates different approaches to text classification. Just take note that this lab environment has compute constraints so you may need to train your model in a different machine if you decide to use one that relies heavily on a GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811e7487-f81c-412a-b5cb-d4f41b366c80",
   "metadata": {},
   "source": [
    "<a id='Deployment'></a>\n",
    "## Deployment\n",
    "\n",
    "Now that you've cleaned the data and retrained the model prototype, you want to deploy it and start receiving new requests. This opens up new challenges that includes both software engineering and machine learning issues. You will have to monitor potential problems and make sure that your model stays performant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d77b9fa-112f-447e-8a29-cd6ee5718953",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531e5eb8-0bd1-48da-b846-1b46478ff25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages you will need in the Deployment Stage\n",
    "\n",
    "import lab_utils\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import requests\n",
    "import json\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad1c333-0010-4dcd-8698-07cf4520e83f",
   "metadata": {},
   "source": [
    "### Deploy in Production\n",
    "\n",
    "The team is using a cloud platform and the prototype model processes news articles in batches every hour. You can see a high level diagram of the system below:\n",
    "\n",
    "<img src='assets/network.png' alt='network.png'>\n",
    "\n",
    "Before you examine the setup and make recommendations, let's first deploy one of the models locally. In Week 1, you used FastAPI, and for this local demo, you will use [Tensorflow Serving](https://www.tensorflow.org/tfx/guide/serving) as your deployment framework. Let's do a quick start and make sample predictions. \n",
    "\n",
    "You will use the `serving` folder in your workspace to contain your models.\n",
    "\n",
    "<details>\n",
    "  <summary><font size=\"2\" color=\"green\"><b>Click here if you're running this notebook outside Coursera</b></font></summary>\n",
    "  <br>\n",
    "\n",
    "_**NOTE:** Tensorflow Serving is already installed in this lab environment. However, if you are running this notebook on your own device, you should install the software first following [the instructions here](https://www.tensorflow.org/tfx/serving/setup). The TF-Serving commands below are for using the `tensorflow-model-server` binary installed using APT. \n",
    "\n",
    "If you opt to use Docker instead, we also placed commands for that in collapsible markdown above the `tensorflow_model_server` commands. You can copy-paste the commands on a new cell to run TF Serving._\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a47e471-e803-4038-9916-8e91a023188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment the lines below if you ran this section before and want to DELETE all models in the serving directory\n",
    "# SERVING_DIR = f'{os.getcwd()}/serving'\n",
    "# os.environ[\"SERVING_DIR\"] = SERVING_DIR\n",
    "# os.system('find $SERVING_DIR -maxdepth 1 -mindepth 1 -type d -exec rm -rf {} \\;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899fd6ff-e046-42e4-80aa-d30269d8d57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVING_DIR = f'{os.getcwd()}/serving'\n",
    "os.environ[\"SERVING_DIR\"] = SERVING_DIR\n",
    "\n",
    "print(f'SERVING_DIR: {SERVING_DIR}')\n",
    "print(f'os.environ[\"SERVING_DIR\"]: {os.environ[\"SERVING_DIR\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008e8814-ad3b-4d73-a115-6b6ebd0af873",
   "metadata": {},
   "source": [
    "Now copy the model in `E2` into the serving directory under a folder named `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7ea33e-1360-448f-b535-644dedb0396e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f'{SERVING_DIR}/1', exist_ok=True)\n",
    "shutil.copytree('./E2/model/', f'{SERVING_DIR}/1', dirs_exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ca48c9-58f1-4235-b7c9-f5618243272e",
   "metadata": {},
   "source": [
    "You can then run Tensorflow Serving in the background and it will detect the model available for serving.\n",
    "\n",
    "<details>\n",
    "  <summary><font size=\"2\" color=\"green\"><b>Click here if you're running this notebook outside Coursera and want to use Docker instead. </b></font></summary>\n",
    "  <br>\n",
    "\n",
    "```\n",
    "## NOTE: Copy and uncomment this on a new cell. This will NOT run on Coursera.\n",
    "\n",
    "# command = (\n",
    "#     'docker run -p 8501:8501 --mount type=bind,source=\"${SERVING_DIR}\",target=/models/newsapp_model '+\n",
    "#     '-e MODEL_NAME=newsapp_model --name=tensorflow-serving -t tensorflow/serving &'\n",
    "# )\n",
    "\n",
    "# os.system(command\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142380a5-14aa-4e18-9ffa-2fc14b34d965",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash --bg \n",
    "nohup tensorflow_model_server \\\n",
    "  --rest_api_port=8501 \\\n",
    "  --model_name=newsapp_model \\\n",
    "  --model_base_path=\"${SERVING_DIR}\" > ./serving/server.log 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21786333-eee9-4def-9587-1b467bfa19ba",
   "metadata": {},
   "source": [
    "The Model Server is now listening at port 8501 and you can send a request to it using the command below. This just sends a dummy array of shape `(1,20)` to mimick a title. You will get a response with the predictions. This will be 8 values corresponding to the output of the softmax dense layer.\n",
    "\n",
    "You can also type the same command (but without the `!`) in a Terminal (Select File -> New Launcher -> Other -> Terminal) and see that the model is indeed accessible outside this notebook.\n",
    "\n",
    "_**NOTE:** If you get an error below about expecting a string input, it's likely that you've ran this lab before and have several models in the `serving` directory. You can scroll up on the code cell below the `Deploy in Production` section, and uncomment the line to remove the existing folders. You will then need to copy the model again to the serving directory before running the command below._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6eeb6b-1415-4b29-96ab-99ea9ea6bcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "command = (\n",
    "    'curl -d \\'{\"instances\": [[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]]}\\' \\\\' +\n",
    "    '-X POST http://localhost:8501/v1/models/newsapp_model:predict'\n",
    ")\n",
    "\n",
    "os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecf099b-27b4-4eab-8e47-a2bd966dc726",
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can also type this command (but without the !) in a Terminal (Select File -> New Launcher -> Other -> Terminal) \n",
    "## and see that the model is indeed accessible outside this notebook.\n",
    "\n",
    "# !curl -d '{\"instances\": [[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]]}' -X POST http://localhost:8501/v1/models/newsapp_model:predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66536bf7-42dd-43c8-a833-b7490eb44e40",
   "metadata": {},
   "source": [
    "You can also send requests programmatically with the code block below. Here, you will load the title preprocessor to convert raw strings to an array that the model will accept. This will work but there's a potential problem here. See if you can spot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ea4119-c85f-420c-8e4a-d7c299cbf914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used by the team on their prototype\n",
    "MAX_LENGTH = 20\n",
    "VOCAB_SIZE = 10000\n",
    "\n",
    "# Instantiate a layer for text preprocessing\n",
    "title_preprocessor = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE, output_sequence_length=MAX_LENGTH)\n",
    "\n",
    "# Load the vocabulary file\n",
    "title_preprocessor.load_assets('./E2/vocab')\n",
    "\n",
    "# Sample input\n",
    "sample_input = 'Sample title'\n",
    "\n",
    "# Preprocess the string\n",
    "sample_input_ds = title_preprocessor(sample_input)\n",
    "\n",
    "# Add a batch dimension\n",
    "sample_input_ds = tf.expand_dims(sample_input_ds, axis=0)\n",
    "\n",
    "# Compose the data\n",
    "data = json.dumps({\"instances\": sample_input_ds.numpy().tolist()})\n",
    "\n",
    "# Define the headers\n",
    "headers = {\"content-type\": \"application/json\"}\n",
    "\n",
    "# Send the request\n",
    "json_response = requests.post('http://localhost:8501/v1/models/newsapp_model:predict', data=data, headers=headers)\n",
    "\n",
    "# Get the predictions\n",
    "predictions = json.loads(json_response.text)['predictions']\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef000eb3-9d32-4384-887a-8464acfe13fb",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "One of the things you want to ensure in serving models is the data transformations. It has to follow the same pipeline as your training data. The problem with the code above is it runs the risk of making a mistake in loading the vocabulary. If you were doing several experiments and edited the code above, you might accidentally load a different vocabulary file (e.g. from Experiment 3 instead of 2). That can make a well-trained model have unreliable predictions because it's preprocessing the titles differently.\n",
    "<br>\n",
    "<br>\n",
    "Different frameworks and tools have a way to avoid this. Since you're using a Tensorflow model, one way is to attach the title preprocessor directly to the input of the trained model. That way, you're sure that the correct vocabulary is already tied to the model whenever you use it. You will do that below and see that you can just pass a string directly to the model instead of an integer array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63e286d-ceb5-49db-b5ed-2e83c76dfa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# working folder for the experiment\n",
    "BASE_DIR = './E2'\n",
    "\n",
    "# get the subdirectories that contain the experiment files\n",
    "data_dir, model_dir, vocab_dir = lab_utils.set_experiment_dirs(BASE_DIR)\n",
    "\n",
    "# Load the model\n",
    "model = tf.keras.models.load_model(model_dir)\n",
    "\n",
    "# Instantiate a layer for text preprocessing\n",
    "title_preprocessor = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE, output_sequence_length=MAX_LENGTH)\n",
    "\n",
    "# Load the vocabulary file\n",
    "title_preprocessor.load_assets(vocab_dir)\n",
    "\n",
    "# Attach the preprocessing layer to the trained model\n",
    "model_with_preprocessor = tf.keras.Sequential([\n",
    "    title_preprocessor,\n",
    "    model\n",
    "])\n",
    "\n",
    "# String input\n",
    "sample_input = \"Sample Title\"\n",
    "\n",
    "# Feed the string input directly to the model\n",
    "model_with_preprocessor.predict([sample_input])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6953984-185d-4924-8cbb-a2055174c51a",
   "metadata": {},
   "source": [
    "Take note that this is usually done after you've trained the model and are ready to deploy. If you attach it before that, it will likely slow down the training especially if you're using GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740135d4-aec4-4c1a-963c-2c786cccaea5",
   "metadata": {},
   "source": [
    "You can now deploy this version of the model as well. Aside from `model.save()`, you can also use [`model.export()`](https://www.tensorflow.org/guide/keras/serialization_and_saving#exporting) which saves a lightweight version of the model dedicated for serving. Run the cell below to save this in the serving directory under the `2` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5193c5b-fb88-4ed2-820d-782293f306bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_preprocessor.export(f'{SERVING_DIR}/2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acbc36d-60c8-448f-91b1-24f2121255c5",
   "metadata": {},
   "source": [
    "Tensorflow Serving will detect this latest model and you can see below that you can just send string inputs to the model. _If you get an error, please wait one minute before running the cell again_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d7bd23-3da3-4ad6-9e37-6fd93cadf78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.dumps({\"instances\": [\"sample title\"]})\n",
    "\n",
    "headers = {\"content-type\": \"application/json\"}\n",
    "json_response = requests.post('http://localhost:8501/v1/models/newsapp_model:predict', data=data, headers=headers)\n",
    "predictions = json.loads(json_response.text)['predictions']\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e143325-c5aa-4fc5-83d6-38cb4a976640",
   "metadata": {},
   "source": [
    "Turn off the Model Server for now before moving to the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f09c85-1a1a-48e5-ac5f-22edac15e162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NOTE: Uncomment and run this cell if you're running on your own device and want to use Docker instead.\n",
    "# !docker stop tensorflow-serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a778e67-d3a6-40d5-8103-dff2d6dd76bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill $(ps aux | grep 'tensorflow_model_server' | awk '{print $2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfc8b3d-09cb-4eff-b018-7ab6896fa417",
   "metadata": {},
   "source": [
    "### Deployment Patterns\n",
    "\n",
    "#### Blue Green Deployment and Canary Deployment\n",
    "\n",
    "Having a way to serve different versions of your model can be useful for easy roll back (blue green deployment) or if you want to gradually switch to a newer version (canary deployment). For TF Serving, this is done through a `model_config_file`. You can open the `models.config` file under the serving directory to see how it maps different models inside a directory to versions and labels. Let's start the server again with these settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c541f875-e74f-48a0-a836-26d22e9ccaf5",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><font size=\"2\" color=\"green\"><b>Click here if you're running this notebook outside Coursera and want to use Docker instead. </b></font></summary>\n",
    "  <br>\n",
    "\n",
    "```\n",
    "## NOTE: Copy and uncomment this on a new cell. This will NOT run on Coursera.\n",
    "\n",
    "# command = (\n",
    "#     'docker run -p 8501:8501 --mount type=bind,source=\"${SERVING_DIR}\",target=/models/newsapp_model ' +\n",
    "#     '--mount type=bind,source=\"${SERVING_DIR}/models.config-docker\",target=/models/models.config ' + \n",
    "#     '-e MODEL_NAME=newsapp_model --name=tensorflow-serving-models-config -t tensorflow/serving ' +\n",
    "#     '--model_config_file=/models/models.config --allow_version_labels_for_unavailable_models=true &'\n",
    "# )\n",
    "\n",
    "# os.system(command)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93013405-e4af-4ccb-8767-51501575b3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash --bg \n",
    "nohup tensorflow_model_server \\\n",
    "  --rest_api_port=8501 \\\n",
    "  --model_config_file=\"${SERVING_DIR}/models.config\" \\\n",
    "  --model_config_file_poll_wait_seconds=10 \\\n",
    "  --allow_version_labels_for_unavailable_models=true \\\n",
    "  --model_base_path=\"${SERVING_DIR}\" > ./serving/server.log 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbbec4e-82f2-46fe-ac70-66f3f1db6983",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "Now you can send requests to the 1st model that has no preprocessor (labeled as \"deprecated\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be68a233-d4a6-487b-9ff4-d0d7aa5482b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.dumps({\"instances\": [[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]]})\n",
    "\n",
    "headers = {\"content-type\": \"application/json\"}\n",
    "json_response = requests.post('http://localhost:8501/v1/models/newsapp_model/labels/deprecated:predict', data=data, headers=headers)\n",
    "predictions = json.loads(json_response.text)['predictions']\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2a4629-19d4-445e-a7c6-f7aecf4fe26d",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "And to the 2nd model as well that accepts string inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b3a33f-9ad9-4205-a479-cee72ff62ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.dumps({\"instances\": [\"sample title 1\", \"sample title 2\"]})\n",
    "\n",
    "headers = {\"content-type\": \"application/json\"}\n",
    "json_response = requests.post('http://localhost:8501/v1/models/newsapp_model/labels/stable:predict', data=data, headers=headers)\n",
    "predictions = json.loads(json_response.text)['predictions']\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ccbb8b-176b-43d2-8338-4facea36fa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE: Deploy the model in Experiment 3. Label it as \"canary\" and try sending string inputs to it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105f4f1a-17fc-4a01-813c-7c6906c519ea",
   "metadata": {},
   "source": [
    "#### Degrees of Automation\n",
    "\n",
    "Another thing to consider is how the ML model will assist the system. The prototype might be deployed now for full automation but based on its current performance, you can decide that it's not ready for that yet. Instead of taking it offline altogether, you can suggest to have a human-in-the-loop and have partial automation instead. \n",
    "\n",
    "For example, in this scenario, you did error analysis and observed that the model is reliable if the probability of the top prediction is 60% or more. If it's less than that, the system will forward that to a human and they will classify that particular article manually.\n",
    "\n",
    "You will do a proof-of-concept of that below using 100 rows from the dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8a2ddf-1ebb-479c-bf1f-9db949d8f1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# working folder for the experiment\n",
    "BASE_DIR = './E2'\n",
    "\n",
    "# get the subdirectories that contain the experiment files\n",
    "data_dir, _, vocab_dir = lab_utils.set_experiment_dirs(BASE_DIR)\n",
    "\n",
    "# Load the dev set\n",
    "dev_df = pd.read_csv(f'{data_dir}/dev_data.csv')\n",
    "\n",
    "# Create a lookup list for the labels\n",
    "topic_lookup = tf.keras.layers.StringLookup(vocabulary=f'{vocab_dir}/labels.txt', num_oov_indices=0)\n",
    "\n",
    "# Get 100 rows from the dev set and prepare the titles as a list\n",
    "title_df = dev_df['title'][:100].reset_index(drop=True)\n",
    "\n",
    "# Convert to a list\n",
    "dev_np = title_df.to_numpy().tolist()\n",
    "\n",
    "# Convert to a JSON object\n",
    "data = json.dumps({\"instances\": dev_np})\n",
    "\n",
    "# Set the header\n",
    "headers = {\"content-type\": \"application/json\"}\n",
    "\n",
    "# Send to the deployed model\n",
    "json_response = requests.post('http://localhost:8501/v1/models/newsapp_model/labels/stable:predict', data=data, headers=headers)\n",
    "\n",
    "# Get the predictions\n",
    "predictions = json.loads(json_response.text)['predictions']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3bda1a-844d-4db4-b3b7-4d93895403cd",
   "metadata": {},
   "source": [
    "You can preview the results as a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9922e088-839d-4971-b9f9-47695a0dd591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the results\n",
    "\n",
    "# Show the predictions as percentages\n",
    "pd.options.display.float_format = '{:.2%}'.format\n",
    "\n",
    "# Format the results as a dataframe\n",
    "pred_df = pd.DataFrame(predictions, columns=topic_lookup.get_vocabulary())\n",
    "pred_df = pd.concat([title_df, pred_df], axis=1)\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211c68e0-a444-4aa4-93bc-beee80dffb5d",
   "metadata": {},
   "source": [
    "Here's a quick demo of filtering the results. If the top prediction exceeds 60%, the article will be classified automatically and forwarded to the app's frontend. Otherwise, they will need to be checked manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77df5fe4-fc18-4693-bcc6-e3915a74a5f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Threshold to meet for automation\n",
    "THRESHOLD = 0.6\n",
    "\n",
    "# List for manual checking\n",
    "below_threshold = []\n",
    "\n",
    "# Collect rows that don't meet the threshold\n",
    "for i, prediction in enumerate(predictions):\n",
    "    if max(prediction) < THRESHOLD:\n",
    "        prediction = prediction.copy()\n",
    "        prediction.insert(0, dev_np[i])\n",
    "        below_threshold.append(prediction)\n",
    "\n",
    "# Preview the results. These will be classified manually.\n",
    "columns = topic_lookup.get_vocabulary()\n",
    "columns.insert(0, 'title')\n",
    "pd.DataFrame(below_threshold, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f105cf-0d2b-4bd9-8268-5a391fd1dfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE: Examine the data and come up with your own filter for manual checking.\n",
    "# Save those that pass the filter in an `auto.csv` file, otherwise save to `manual.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa35416-bd62-49a7-a063-7ec475162019",
   "metadata": {},
   "source": [
    "### Monitor and Maintain System\n",
    "\n",
    "Deploying an ML system is not the end of a project. Over time, its performance will degrade as gradual change or sudden shock makes the incoming data less like those that the model was trained on. One problem is concept drift where the relationship between the inputs and outputs change. For example, an athlete retires and becomes more known as a celebrity but your model still always predicts `SPORTS` for that person. Another is data drift where the input distribution changes. For example, a person suddenly becomes famous but if there were no related articles in the training data, your model can only use the other words in the new titles to deduce the topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30454b0f-328a-49e1-9cba-18d47ab08813",
   "metadata": {},
   "source": [
    "#### Metrics and Alarms\n",
    "\n",
    "When your model goes live in production, you will want to know as early as possible if there's a problem. You can sit with your team and brainstorm about things that might go wrong, and come up with metrics to detect them. You can setup an alarm if a threshold if met so you can investigate. Metrics are broadly divided into three types:\n",
    "\n",
    "* Software metrics - These refer to the capacity of the platform to deliver your prediction service. It includes measuring available memory and compute resources, latency, throughput, and server load. These will let you know if you need to scale up or scale down the resources to keep your system running smoothly.\n",
    "\n",
    "* Input metrics - These refer to the characteristics of the incoming data. You can use these to detect if your input distribution is changing or if there are edge cases that you might have not considered. In your news classifier, some metrics you can consider are: average input length, number of unknown tokens, out-of-vocabulary words, number of requests from each source, etc.\n",
    "\n",
    "* Output metrics - These refer to the results of the model predictions. Some examples are: average number of predictions per news category, ratio of automated to manual predictions, average probability per prediction, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ef27ee-9767-4e4e-a6cb-60b55c462379",
   "metadata": {},
   "source": [
    "Let's demonstrate one scenario below where you will monitor the number of unknown tokens (i.e. out-of-vocabulary words) in the inputs. This is one metric you want to consider because having a lot of them means that the vocabulary your model is using is insufficient. You will grab the entries available in the database and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28154bd5-3624-46cc-8e79-b945a2fa719c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the database\n",
    "con = sqlite3.connect(\"news_articles.db\")\n",
    "cur = con.cursor()\n",
    "\n",
    "# Preview some entries\n",
    "for row in cur.execute(\"SELECT id,title FROM news_articles WHERE id < 5\"):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00179cea-48d0-48a6-8a58-de7ce879949f",
   "metadata": {},
   "source": [
    "You will then load the title preprocessor in Experiment 2 so you can transform the string inputs into integer sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6dc587-51fd-4c20-932a-5fed6f4a6519",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE=10000\n",
    "MAX_LENGTH=20\n",
    "\n",
    "# working folder for the experiment\n",
    "BASE_DIR = './E1'\n",
    "\n",
    "# get the subdirectories that contain the experiment files\n",
    "_, model_dir, vocab_dir = lab_utils.set_experiment_dirs(BASE_DIR)\n",
    "\n",
    "# Load the model\n",
    "model = tf.keras.models.load_model(model_dir)\n",
    "\n",
    "# Instantiate a layer for text preprocessing\n",
    "title_preprocessor = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE, output_sequence_length=MAX_LENGTH)\n",
    "\n",
    "# Load the vocabulary file\n",
    "title_preprocessor.load_assets(vocab_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f74a6f-fdee-4182-acfa-adb113892f4d",
   "metadata": {},
   "source": [
    "For each integer sequence, you want to count the number of `1`'s because that represents an out-of-vocabulary word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3645d2c-564c-474a-812c-f0035c03cf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize list containing the number of `1`'s per title\n",
    "unk_counts = []\n",
    "\n",
    "# Iterate over the titles in the database\n",
    "for row in cur.execute(\"SELECT title FROM news_articles\"):\n",
    "\n",
    "    # Convert the title to an integer sequence\n",
    "    sequence = title_preprocessor(row[0])\n",
    "\n",
    "    # Count the number of `1`\n",
    "    unk_count = np.count_nonzero(sequence == 1)\n",
    "\n",
    "    # Append to the list\n",
    "    unk_counts.append(unk_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8536db9b-aaa4-4e7b-af31-adc0dbd63be7",
   "metadata": {},
   "source": [
    "Now plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e041f32-e1ad-443f-bb92-75651a3c85fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ids\n",
    "ids = range(1, len(unk_counts)+1)\n",
    "\n",
    "# Plot the ids and unknown token counts\n",
    "plt.plot(ids, unk_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be035835-2380-4c30-97a7-c09a6dc92507",
   "metadata": {},
   "source": [
    "Notice that there is a spike in the later entries. You can inspect these rows and see what might be the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fa3e60-6597-4553-85c1-28bbf8e4c3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print rows after id=100\n",
    "for row in cur.execute(\"SELECT id,title FROM news_articles WHERE ID > 100\"):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f9dfb8-07b5-4268-aaf3-b47dfea7fcb1",
   "metadata": {},
   "source": [
    "As you can see, there's an input error here. Your model was trained only on English articles so it wasn't able to recognize any of the words used in these titles. With this insight, you can act on it so it will not negatively impact your application. You can find the news source and filter entries like this so it won't mistakenly go into your users' news application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7770fe1c-dadc-4d19-9161-bb3a1befb21f",
   "metadata": {},
   "source": [
    "## Wrap Up\n",
    "\n",
    "Great work completing this project. You've successfully applied all the steps of the Machine Learning Project lifecycle to a real-world project, from scoping to data to modeling to deployment. You tackled common challenges like fixing unbalanced train/dev/test splits, addressing labeling ambiguity, and deploying a model into production.\n",
    "\n",
    "The Machine Learning Project Lifecycle is an iterative process and you might have found yourself jumping from one stage to another to produce better results for your project. Given your findings so far, you can summarize your recommendations to the product owner so they can successfully integrate ML into their news app. Here are some aspects you might like to take note of:\n",
    "\n",
    "* How would you reduce label ambiguity when you ask human labellers to provide the ground truth?\n",
    "* What is the highest accuracy you got with the prototype model and will it be sufficient for the application?\n",
    "* What are the timeline and resources needed to carry out the next phase of the project?\n",
    "* What modifications can you suggest to the app? For example, can an article appear across different categories?\n",
    "\n",
    "We encourage you to share and discuss your findings on [the DLAI Forum](https://community.deeplearning.ai/c/course-q-a/machine-learning-engineering-for-production/mlep-learner-projects/224). You can post about anything from a good labeling strategy to a state-of-the-art model built for this kind of task. You can also apply what you learned here to a different dataset and showcase it as your personal project. We're excited to see what you build!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dfca43-74f1-45bd-bdc7-3a7a86e794e9",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "**Acknowledgement:**\n",
    "\n",
    "The dataset used in this scenario is a revised version of the [Topic Labeled News Dataset](https://www.kaggle.com/datasets/kotartemiy/topic-labeled-news-dataset)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
